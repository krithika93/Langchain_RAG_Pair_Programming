{"cells":[{"cell_type":"code","source":["Tetst tRnasform and load data analytics , Data science Laod Tex, vector Trrnas AI API MADS , LTesong Foe"],"metadata":{"id":"UyVQo9ko8LBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIRItwu0yap1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706132719643,"user_tz":360,"elapsed":123659,"user":{"displayName":"Krithika Rajendran","userId":"08824665399023076954"}},"outputId":"5935878b-c7f9-4dd2-e083-3346ff240269"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.5/2.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/2.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install \"shapely<2.0.0\" --quiet\n","%pip install cohere --quiet\n","%pip install tiktoken --quiet\n","%pip install langchain --quiet\n","%pip install google-cloud-aiplatform --upgrade --quiet\n","%pip install gradio --quiet\n","%pip install openai==0.28 --quiet\n","%pip install streamlit --quiet\n","%pip install streamlit-ace --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exLERtQTye5H"},"outputs":[],"source":["from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n","from langchain.chat_models.vertexai import ChatVertexAI\n","from langchain.schema.output_parser import StrOutputParser\n","from langchain.schema.runnable import RunnableLambda\n","from langchain.memory import ConversationBufferMemory\n","from streamlit_ace import st_ace\n","import gradio as gr\n","import streamlit as st\n","import vertexai\n","import openai\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVJznCsDz2LT"},"outputs":[],"source":["from google.colab import auth as google_auth\n","google_auth.authenticate_user()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnV0kJlKz3rX"},"outputs":[],"source":["vertexai.init(project=\"project-radicalx\", location=\"us-central1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtU1wRE1ztDp"},"outputs":[],"source":["intent_chain = PromptTemplate.from_template('''You are helping a user solve a coding problem, provided below.\n","\n","Given the user message, classify its intent as one of the following categories: `asking for problem solving strategy`, `asking for review of user's code`, `general Python question`, `other`.\n","\n","Respond with the topic only and no other words.\n","\n","<coding_problem>\n","{coding_problem}\n","</coding_problem>\n","\n","<message>\n","{message}\n","</message>\n","\n","Classification''') | ChatVertexAI() | StrOutputParser()\n","\n","collab_chain = ChatPromptTemplate.from_messages([\n","    ('system', '''You are working with a user on their Python problem, provided below. They have also written some code. Assume you don't know how to solve the problem.\n","\n","Please comment on the user message, using emojis where appropriate.\n","If the user talks about their code, please take a look at their code as well. Be vague - don't give any big hints, next steps, solutions, or the answer.\n","\n","If you give a hint, only give a single step do not give all the steps at once.\n","\n","Keep your response 1-2 sentences maximum. Remember - talk like you are a coding partner and not the user's teacher. Don't give any code.\n","\n","<coding_problem>\n","{coding_problem}\n","</coding_problem>\n","\n","<user_code>\n","{user_code}\n","</user_code>\n","\n","<code_output>\n","{code_output}\n","</code_output>'''),\n","    MessagesPlaceholder(variable_name='chat_history'),\n","    ('human', '{message}')]) | ChatVertexAI()\n","\n","code_chain = ChatPromptTemplate.from_messages([\n","    ('system', '''You are working with a user on their Python problem, provided below. They have also written some code. Assume you don't know how to solve the problem.\n","\n","Please provide a small snippet of code relevant to the user message. Keep it short and to the point. Don't give any hints, solutions, or the answer.\n","\n","Use emojis where appropriate. Remember - talk like you are a coding partner and not the user's teacher.\n","\n","<coding_problem>\n","{coding_problem}\n","</coding_problem>\n","\n","<user_code>\n","{user_code}\n","</user_code>'''),\n","    MessagesPlaceholder(variable_name='chat_history'),\n","    ('human', '{message}')]) | ChatVertexAI()\n","\n","default_chain = ChatPromptTemplate.from_messages([\n","    ('system', '''You are a junior coder helping a user solve a Python coding problem. Reply back to the user's message using plenty of appropriate emojis.\n","\n","Keep your answer short and to the point. Don't ask follow-up questions.'''),\n","    MessagesPlaceholder(variable_name='chat_history'),\n","    ('human', '{message}')]) | ChatVertexAI()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pG-3uunt-38H"},"outputs":[],"source":["def response_route(info):\n","  if 'asking for problem solving strategy' in info['intent'].lower():\n","      return 'STRATEGY\\n' + str(collab_chain.invoke(info).content)\n","      #return str(collab_chain.invoke(info).content)\n","  elif f'''asking for review of user's code''' in info['intent'].lower():\n","      return f'''Sounds like you're asking me for a code review/debug'''\n","  elif 'general Python question' in info['intent'].lower():\n","      return 'GENERAL CODING\\n' + str(code_chain.invoke(info).content)\n","      #return str(code_chain.invoke(info).content)\n","  else:\n","      return 'DEFAULT\\n' + str(default_chain.invoke(info).content)\n","      #return str(default_chain.invoke(info).content)\n","\n","response_chain = {\n","    'intent': intent_chain,\n","    'message': lambda x: x['message'],\n","    'coding_problem': lambda x: x['coding_problem'],\n","    'user_code': lambda x: x['user_code'],\n","    'code_output': lambda x: x['code_output'],\n","    'chat_history': lambda x: x['chat_history']\n","} | RunnableLambda(response_route)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNCYJ762JXCq"},"outputs":[],"source":["screen_chain = PromptTemplate.from_template('''A coding problem is provided below, as 'coding problem'.\n","\n","Given 'response', determine if 'response' meets any of the following criteria:\n","\n","- Gives the solution to 'coding problem'\n","- Not just a small hint, but gives the whole strategy for solving 'coding problem'\n","- Providing code to specifically solve 'coding problem'\n","\n","Small talk is fine. Respond with a single character, either 'Y' for yes or 'N' for no.\n","\n","<coding_problem>\n","{coding_problem}\n","</coding_problem>\n","\n","<response>\n","{response}\n","</response>\n","\n","Classification''') | ChatVertexAI() | StrOutputParser()\n","\n","def output_route(info):\n","  if 'n' in info['screen'].lower():\n","      return info['response']\n","  elif 'y' in info['screen'].lower():\n","      #debug_msg = 'SCREENED OUT\\n' + info['response'] + '\\n'\n","      debug_msg = ''\n","      return debug_msg + f'''I'm sorry, I don't have a good response to that. You can say it in a different way or tell me something else :)'''\n","  else:\n","      return 'There was an error with the output screening'\n","\n","output_chain = {\n","    'screen': screen_chain,\n","    'message': lambda x: x['message'],\n","    'coding_problem': lambda x: x['coding_problem'],\n","    'response': lambda x: x['response'],\n","} | RunnableLambda(output_route)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFMF58JJoOOH"},"outputs":[],"source":["#API Key\n","OPENAI_API_KEY=\"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\"\n","openai.api_key=OPENAI_API_KEY\n","\n","def get_coding_problem():\n","  #Prompt Template\n","  prompt_template = f\"\"\"\n","  Please generate a coding question with one test input.\n","  Format your response as a JSON object with the following 2 keys:\n","\n","  \"Description\": A text description of the coding question\n","  \"Input\": This should contain a dictionary where the key-value pairs are test input variable names and their values\n","\n","  Do not include any additional information like examples, outputs, hints.\n","  \"\"\"\n","\n","  #Response\n","  response = openai.Completion.create(\n","      engine=\"text-davinci-003\",\n","      prompt=prompt_template,\n","      max_tokens=1024,\n","      n=1,\n","      stop=None,\n","      temperature=1,\n","  )\n","  return response[\"choices\"][0][\"text\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HENy9j0BpA4i"},"outputs":[],"source":["# lookup = json.loads(get_coding_problem())\n","\n","# prob_description = lookup['Description']\n","# prob_inputs = ''\n","\n","# for var_name, var_val in lookup['Input'].items():\n","#   prob_inputs += f'''{var_name} = {var_val}\\n'''\n","\n","# print(prob_description)\n","# print(prob_inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1knO1GkEN0E"},"outputs":[],"source":["lookup = json.loads(get_coding_problem())\n","\n","prob_description = lookup['Description']\n","prob_inputs = ''\n","\n","for var_name, var_val in lookup['Input'].items():\n","  prob_inputs += f'''{var_name} = {var_val}\\n'''\n","\n","global coding_problem\n","coding_problem = f'''\n","Description:\n","{prob_description}\n","\n","Inputs:\n","{prob_inputs}\n","'''\n","\n","memory = ConversationBufferMemory(return_messages=True)\n","memory.save_context({'input': f'''Let's practice coding in Python.'''}, {'output': f'''Let's do it! Here's a problem that I found:\\n{coding_problem}'''})\n","\n","with gr.Blocks() as demo:\n","\n","  def get_completion(msg, user_code, code_output, script):\n","    global coding_problem\n","\n","    response_inputs = {'coding_problem': coding_problem, 'message': msg, 'user_code': user_code, 'code_output': code_output, 'chat_history': memory.load_memory_variables({})['history']}\n","    response = response_chain.invoke(response_inputs)\n","    output = output_chain.invoke({'coding_problem': coding_problem, 'message': msg, 'response': response})\n","\n","    memory.save_context({'input': msg}, {'output': output})\n","    script.append((msg, output))\n","\n","    return '', script\n","\n","  def run_code(code):\n","    try:\n","        local_vars = {}\n","        exec(code, None, local_vars)\n","\n","        out_string = 'Your code ran successfully!\\n'\n","        for var, val in local_vars.items():\n","            out_string += f'''\\n{var} = {val}'''\n","\n","        return out_string\n","\n","    except Exception as e:\n","        return 'ERROR\\n' + str(e)\n","\n","  with gr.Row():\n","    with gr.Column():\n","      chatbot = gr.Chatbot([(f'''Let's practice coding in Python.''', f'''Let's do it! Here's a problem that I found:\\n{coding_problem}''')])\n","      chat_input = gr.Textbox(value='', label='Chat with junior coder')\n","\n","    with gr.Column():\n","      code = gr.Code(prob_inputs)\n","      code_submit = gr.Button(value='Run')\n","      code_output = gr.Textbox(value='', label='Code Output')\n","\n","  chat_input.submit(get_completion, [chat_input, code, code_output, chatbot], [chat_input, chatbot])\n","  code_submit.click(run_code, [code], [code_output])\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"PBIQGuepNmFc"},"source":["**STREAMLIT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWvYLLDmKdUL"},"outputs":[],"source":["import streamlit as st\n","\n","#Generate APIs as fxns\n","\n","col1, col2 = st.columns(2)\n","\n","def main():\n","\n","    with col1:\n","        s=st_ace\n","        coding_area=st.text_area('Workbench')\n","\n","    with col2:\n","        l=st.header('Results'),\n","        output=st.text_input('Output'),\n","        solution=st.text_input('Solution'),\n","\n","\n","# Just add it after st.sidebar:\n","q=st.sidebar.header('Question')\n","\n","a=st.sidebar.code('for i in range(8): foo()')\n","\n","a=st.sidebar.text_area(\"Hello, I am your Coding Companion. How Can I help!!汨欺")\n","\n","b=st.button(\"Compile\")\n","# from code_editor import code_editor\n","# response_dict = code_editor(\"\")\n","\n","st.title(\"Simple Chatbot with GPT-3\")\n","st.markdown(\"\"\"\n","        ## Chat with the Chatbot\n","        \"\"\")\n","user_input = st.text_input(\"You\", value=\"\", key=1)\n","\n","if st.button(\"Send\", key=1):\n","    if user_input:\n","            st.text_area(\"Chatbot\", value=get_coding_problem(user_input), key=2)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQlcpmPpuxOt"},"outputs":[],"source":["!npm install localtunnel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1700205797228,"user":{"displayName":"Jai Patel","userId":"03197586106342298935"},"user_tz":300},"id":"D3I4YE5gNsE_","outputId":"b6dad39f-c598-41ab-ae1e-e227fa05517e"},"outputs":[{"output_type":"stream","name":"stdout","text":["34.73.130.50\n"]}],"source":["! streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py &>/content/logs.txt & curl ipv4.icanhazip.com"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nB9GG_IjuR7x","outputId":"16da5bd6-8d44-4d2c-aab1-da616b2fae5a","executionInfo":{"status":"ok","timestamp":1700205874026,"user_tz":300,"elapsed":68713,"user":{"displayName":"Jai Patel","userId":"03197586106342298935"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25hnpx: installed 22 in 1.458s\n","your url is: https://some-results-fail.loca.lt\n","^C\n"]}],"source":["!npx localtunnel --port 8501"]}],"metadata":{"colab":{"provenance":[{"file_id":"1kaa9VdJFefRg9J_HbrOuMygzehy_zqfL","timestamp":1706132580337},{"file_id":"18t3T7qA9XtTUrbjfltn-tYghBsr9Xlzb","timestamp":1698847083387}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}